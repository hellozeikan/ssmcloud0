Centos7配置
hostnamectl set-hostname 主机名    ///修改主机名
ssh免密登录:
ssh-keygen -t rsa　　//默认指定的是rsa，
ls /root/.ssh/    //查看密钥
ssh-copy-id -i .ssh/id_rsa.pub  root@IP //发送公钥
双向免密在后面的机子上重复上面操作即可
wget --no-check-certificate URL //免证书下载
1）创建安装目录
mkdir /usr/local/kafka/
tar -zxvf apache-hive-2.1.1-bin.tar.gz -C /usr/local/hive/
tar -zxvf kafka_2.11-0.11.0.0.tgz -C /usr/local/kafka/
scp -r /usr/local/kafka/kafka_2.11-0.11.0.0/ qiang2:/usr/local/kafka/
卡夫卡创建话题：
bin/kafka-topics.sh --create  --zookeeper localhost:2181   --replication-factor 1  --partitions 1    --topic test2 



mkdir /usr/local/hbase/

hbase-evn.sh
export JAVA_HOME=/usr/local/java/jdk1.8.0_251
export HBASE_MANAGES_ZK=false

scp -r /usr/local/hbase/hbase-1.2.6 qiang1:/usr/local/hbase


create 'message', {NAME => 'attends', VERSIONS => 3}, {NAME =>'fans', BLOCKCACHE => true}

put 'message','000001',




mkdir /usr/local/zookeeper/
mkdir /usr/local/java/
mkdir /usr/local/hadoop/
mkdir /usr/local/hive/
export ZOOKEEPER_HOME=/usr/local/zookeeper/zookeeper-3.4.8
tar -zxvf apache-hive-2.1.1-bin.tar.gz -C /usr/local/hive/
mkdir /usr/local/sqoop/
tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /usr/local/sqoop/
export  SQOOP_HOME=/usr/local/sqoop/sqoop-1.4.7.bin__hadoop-2.6.0
sqoop-env.sh
export  HADOOP_COMMON_HOME=/usr/local/hadoop/hadoop-2.7.7
export  HADOOP_MAPRED_HOME=/usr/local/hadoop/hadoop-2.7.7
export  HIVE_HOME=/usr/local/hive/apache-hive-2.1.1-bin


export  HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export  HIVE_CONF_DIR=/usr/local/hive/apache-hive-2.1.1-bin/conf
export  HIVE_AUX_JARS_PATH=/usr/local/hive/apache-hive-2.1.1-bin/lib

export  HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop 
export  HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native 
export  HADOOP_OPTS="-Djava.library.path=${HADOOP_HOME}/lib" 
export  HIVE_HOME=/usr/local/hive/apache-hive-2.1.1-bin
export  HIVE_CONF_DIR=${HIVE_HOME}/conf 
export  CLASS_PATH=.:${JAVA_HOME}/lib:${HIVE_HOME}/lib:$CLASS_PATH
export  PATH=.:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:$PATH 
（2）解压至安装目录

tar -zxvf jdk-8u251-linux-x64.tar.gz -C /usr/local/java/
打开文件

vi /etc/profile

末尾添加
export JAVA_HOME=/usr/local/java/jdk1.8.0_251
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.7
export PATH=$PATH:$HADOOP_HOME/bin
使环境变量生效
source /etc/profile
给虚拟机起host
vi /etc/hosts
192.168.33.101 qiang1
192.168.33.102 qiang2
192.168.33.103 qiang3
mkdir /usr/local/hadoop/
mkdir /usr/local/sqoop/
解压hadoop
tar -zxvf hadoop-2.7.7.tar.gz  -C /usr/local/hadoop/
配置
vi /usr/local/hadoop/hadoop-2.7.7/etc/hadoop/slaves
vi /usr/local/hadoop/hadoop-2.7.7/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/local/java/jdk1.8.0_251
vi /usr/local/hadoop/hadoop-2.7.7/etc/hadoop/masters
vi /usr/local/hadoop/hadoop-2.7.7/etc/hadoop/core-site.xml
<property>
        <name>hadoop.tmp.dir</name>
        <value>file:///usr/local/hadoop/hadoop-2.7.7</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://qiang1:8888</value>
    </property>



scp -r /usr/local/zookeeper/zookeeper-3.4.8 root@qiang2:/usr/local/zookeeper/


vi /usr/local/hadoop/hadoop-2.7.7/etc/hadoop/hdfs-site.xml
 <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>master:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///usr/local/hadoop/hadoop-2.7.7/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///usr/local/hadoop/hadoop-2.7.7/tmp/dfs/data</value>
    </property>

vi /usr/local/hadoop/hadoop-2.7.7/etc/hadoop/mapred-site.xml
     
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
vi /usr/local/hadoop/hadoop-2.7.7/etc/hadoop/yarn-site.xml
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
scp -r slaves hadoop-env.sh core-site.xml  hdfs-site.xml yarn-site.xml hdfs-site.xml root@qiang2:/usr/local/hadoop/hadoop-2.7.7/etc/hadoop/